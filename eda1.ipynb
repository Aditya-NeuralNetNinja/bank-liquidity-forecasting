{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0",
   "metadata": {},
   "source": [
    "# Payment Transaction Volume Forecasting — EDA\n",
    "\n",
    "**Goal:** Explore the bank transaction dataset, aggregate to daily time series, and perform\n",
    "time series analysis (decomposition, stationarity, ACF/PACF) to prepare for modeling.\n",
    "\n",
    "**Why EDA first?** We need to understand the data's structure — trends, seasonality,\n",
    "stationarity — before choosing the right forecasting model. A SARIMA model needs\n",
    "different prep than XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1",
   "metadata": {},
   "source": [
    "## Step 1: Load & Inspect Raw Data\n",
    "\n",
    "**Why:** Before any transformation, we need to understand what columns exist,\n",
    "their types, null counts, and sample values. This tells us what cleaning is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_raw = pd.read_csv('bank_clean.csv')\n",
    "print(f'Shape: {df_raw.shape}')\n",
    "print(f'\\nColumn types & nulls:')\n",
    "df_raw.info()\n",
    "print(f'\\nDuplicates: {df_raw.duplicated().sum()}')\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning\n",
    "\n",
    "**What we drop and why:**\n",
    "- `cheque_no` — 99.2% null, zero predictive value for aggregate forecasting\n",
    "- `balance_amt` — account-level running balance, not a flow metric\n",
    "\n",
    "**What we keep and why:**\n",
    "- `account_no` — needed to count unique customers per day (dropped AFTER aggregation)\n",
    "- `transaction_details` — needed for transaction type mix ratios (dropped AFTER aggregation)\n",
    "- `value_date` — settlement date = when money actually moves (used for aggregation)\n",
    "- `date` — transaction initiation date (compare with value_date, then drop)\n",
    "- `withdrawal_amt`, `deposit_amt` — our core amount columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: do 'date' and 'value_date' differ?\n",
    "# If they differ, value_date is the correct one for liquidity (settlement date)\n",
    "date_mismatch = (df_raw['date'] != df_raw['value_date']).sum()\n",
    "print(f'Rows where date != value_date: {date_mismatch} out of {len(df_raw)} ({date_mismatch/len(df_raw)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with no forecasting value\n",
    "df = df_raw.drop(columns=['cheque_no', 'balance_amt'])\n",
    "\n",
    "# Drop exact duplicates\n",
    "df = df.drop_duplicates()\n",
    "print(f'Shape after cleaning: {df.shape}')\n",
    "\n",
    "# Fill NaN in amounts with 0 (a row is either a withdrawal OR a deposit, not both)\n",
    "df['withdrawal_amt'] = df['withdrawal_amt'].fillna(0)\n",
    "df['deposit_amt'] = df['deposit_amt'].fillna(0)\n",
    "\n",
    "# Parse dates\n",
    "df['value_date'] = pd.to_datetime(df['value_date'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3",
   "metadata": {},
   "source": [
    "## Step 3: Aggregation Pipeline — Raw Transactions to Daily Time Series\n",
    "\n",
    "**Why:** Our raw data has one row per transaction. We need to aggregate to daily level\n",
    "because we're forecasting *daily* volumes and amounts.\n",
    "\n",
    "**What we compute per day:**\n",
    "- `transaction_volume` = COUNT of all transactions → drives **infrastructure planning**\n",
    "- `deposit_amt` = SUM of deposits → money flowing IN\n",
    "- `withdrawal_amt` = SUM of withdrawals → money flowing OUT\n",
    "- `net_flow` = deposits - withdrawals → drives **liquidity planning**\n",
    "- `unique_accounts` = COUNT of distinct accounts transacting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = (\n",
    "    df\n",
    "    .groupby('value_date')\n",
    "    .agg(\n",
    "        transaction_volume=('value_date', 'size'),\n",
    "        total_withdrawal=('withdrawal_amt', 'sum'),\n",
    "        total_deposit=('deposit_amt', 'sum'),\n",
    "        unique_accounts=('account_no', 'nunique')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Net flow = deposits - withdrawals (positive = surplus, negative = deficit)\n",
    "daily['net_flow'] = daily['total_deposit'] - daily['total_withdrawal']\n",
    "\n",
    "print(f'Daily aggregated shape: {daily.shape}')\n",
    "print(f'Date range: {daily.index.min()} to {daily.index.max()}')\n",
    "daily.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4",
   "metadata": {},
   "source": [
    "## Step 4: Handle Missing Dates\n",
    "\n",
    "**Why:** Time series models (ARIMA, SARIMA) expect a continuous date index with no gaps.\n",
    "If Jan 3 and Jan 5 exist but Jan 4 is missing, the model doesn't know a day was skipped.\n",
    "\n",
    "**Approach:**\n",
    "1. Create a complete date range from min to max date\n",
    "2. Reindex — days with no transactions become NaN\n",
    "3. Fill NaN: volume = 0, amounts = 0 (no transactions means zero, not unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complete date range\n",
    "full_range = pd.date_range(start=daily.index.min(), end=daily.index.max(), freq='D')\n",
    "\n",
    "# How many dates are missing?\n",
    "missing_dates = full_range.difference(daily.index)\n",
    "print(f'Missing dates: {len(missing_dates)} out of {len(full_range)} total days')\n",
    "\n",
    "# Reindex and fill: no transaction = 0 volume, 0 amount\n",
    "daily = daily.reindex(full_range)\n",
    "daily.index.name = 'date'\n",
    "daily[['transaction_volume', 'total_withdrawal', 'total_deposit', 'net_flow', 'unique_accounts']] = \\\n",
    "    daily[['transaction_volume', 'total_withdrawal', 'total_deposit', 'net_flow', 'unique_accounts']].fillna(0)\n",
    "\n",
    "# Convert volume and unique_accounts to int\n",
    "daily['transaction_volume'] = daily['transaction_volume'].astype(int)\n",
    "daily['unique_accounts'] = daily['unique_accounts'].astype(int)\n",
    "\n",
    "print(f'\\nFinal daily shape: {daily.shape}')\n",
    "daily.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Time Series\n",
    "\n",
    "**Why:** Before any statistical test, visually inspect the data.\n",
    "We're looking for:\n",
    "- **Trend** — is volume increasing/decreasing over time?\n",
    "- **Seasonality** — weekly cycles? month-end spikes?\n",
    "- **Outliers** — any extreme days that could distort models?\n",
    "- **Net flow direction** — are there prolonged deficit periods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, figsize=(14, 16), sharex=True)\n",
    "\n",
    "# Transaction Volume\n",
    "axes[0].plot(daily.index, daily['transaction_volume'], color='#FF914D', linewidth=0.8)\n",
    "axes[0].set_title('Daily Transaction Volume (count)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Total Deposits\n",
    "axes[1].plot(daily.index, daily['total_deposit'], color='green', linewidth=0.8)\n",
    "axes[1].set_title('Daily Total Deposits')\n",
    "axes[1].set_ylabel('Amount')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Total Withdrawals\n",
    "axes[2].plot(daily.index, daily['total_withdrawal'], color='red', linewidth=0.8)\n",
    "axes[2].set_title('Daily Total Withdrawals')\n",
    "axes[2].set_ylabel('Amount')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Net Flow\n",
    "axes[3].plot(daily.index, daily['net_flow'], color='#1f77b4', linewidth=0.8)\n",
    "axes[3].axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "axes[3].set_title('Daily Net Flow (Deposit - Withdrawal) → Liquidity Indicator')\n",
    "axes[3].set_ylabel('Net Amount')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b",
   "metadata": {},
   "source": [
    "### Distributional Properties\n",
    "\n",
    "**Why:** Check if transaction volume / amounts are normally distributed or skewed.\n",
    "Skewed data may need log transformation before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "daily['transaction_volume'].hist(bins=50, ax=axes[0], color='#FF914D', edgecolor='black')\n",
    "axes[0].set_title('Distribution: Transaction Volume')\n",
    "\n",
    "daily['total_deposit'].hist(bins=50, ax=axes[1], color='green', edgecolor='black')\n",
    "axes[1].set_title('Distribution: Daily Deposits')\n",
    "\n",
    "daily['net_flow'].hist(bins=50, ax=axes[2], color='#1f77b4', edgecolor='black')\n",
    "axes[2].set_title('Distribution: Net Flow')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Summary Statistics:')\n",
    "daily[['transaction_volume', 'total_deposit', 'total_withdrawal', 'net_flow']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c",
   "metadata": {},
   "source": [
    "### Day-of-Week & Month-End Patterns\n",
    "\n",
    "**Why:** Payment data typically shows strong day-of-week effects (weekends are quieter)\n",
    "and month-end spikes (salary payments, bill settlements). Confirming these patterns\n",
    "tells us seasonality exists and SARIMA / calendar features will be valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "daily['day_of_week'] = daily.index.day_name()\n",
    "daily['is_month_end'] = daily.index.is_month_end\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Volume by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "sns.boxplot(data=daily, x='day_of_week', y='transaction_volume', order=day_order, ax=axes[0],\n",
    "            palette='Oranges')\n",
    "axes[0].set_title('Transaction Volume by Day of Week')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Volume: month-end vs normal days\n",
    "sns.boxplot(data=daily, x='is_month_end', y='transaction_volume', ax=axes[1],\n",
    "            palette='Oranges')\n",
    "axes[1].set_title('Transaction Volume: Month-End vs Normal Days')\n",
    "axes[1].set_xticklabels(['Normal Days', 'Month-End'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up temp columns\n",
    "daily.drop(columns=['day_of_week', 'is_month_end'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6",
   "metadata": {},
   "source": [
    "## Step 6: Time Series Decomposition\n",
    "\n",
    "**Why:** Decomposition separates a time series into 3 components:\n",
    "- **Trend** — long-term increase/decrease\n",
    "- **Seasonality** — repeating patterns (weekly, monthly cycles)\n",
    "- **Residual** — what's left after removing trend + seasonality (noise / irregular events)\n",
    "\n",
    "This tells us:\n",
    "- Does a trend exist? → determines if differencing is needed\n",
    "- How strong is seasonality? → determines if SARIMA (seasonal) is better than ARIMA\n",
    "- Are residuals random? → if not, the model can capture more signal\n",
    "\n",
    "**Two approaches:**\n",
    "1. Classical decomposition (additive) — assumes components add up: Y = Trend + Seasonal + Residual\n",
    "2. STL decomposition — more robust, handles outliers better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
    "\n",
    "# Classical Additive Decomposition on transaction volume\n",
    "# period=7 because we expect weekly seasonality (7 days)\n",
    "decomp = seasonal_decompose(daily['transaction_volume'], model='additive', period=7)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "decomp.observed.plot(ax=axes[0], color='#FF914D')\n",
    "axes[0].set_ylabel('Observed')\n",
    "decomp.trend.plot(ax=axes[1], color='#FF914D')\n",
    "axes[1].set_ylabel('Trend')\n",
    "decomp.seasonal.plot(ax=axes[2], color='#FF914D')\n",
    "axes[2].set_ylabel('Seasonal')\n",
    "decomp.resid.plot(ax=axes[3], color='#FF914D')\n",
    "axes[3].set_ylabel('Residual')\n",
    "\n",
    "fig.suptitle('Classical Additive Decomposition — Daily Transaction Volume (period=7)', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL Decomposition — more robust to outliers\n",
    "# Why STL over classical? STL uses LOESS (locally weighted regression)\n",
    "# so it handles irregular spikes better than moving-average-based classical decomposition\n",
    "\n",
    "stl = STL(daily['transaction_volume'], period=7)\n",
    "result = stl.fit()\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "result.observed.plot(ax=axes[0], color='#FF914D')\n",
    "axes[0].set_ylabel('Observed')\n",
    "result.trend.plot(ax=axes[1], color='#FF914D')\n",
    "axes[1].set_ylabel('Trend')\n",
    "result.seasonal.plot(ax=axes[2], color='#FF914D')\n",
    "axes[2].set_ylabel('Seasonal')\n",
    "result.resid.plot(ax=axes[3], color='#FF914D')\n",
    "axes[3].set_ylabel('Residual')\n",
    "\n",
    "fig.suptitle('STL Decomposition — Daily Transaction Volume (period=7)', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7",
   "metadata": {},
   "source": [
    "## Step 7: Stationarity Testing\n",
    "\n",
    "**Why this matters:** Most time series models (ARIMA, SARIMA) require the data to be\n",
    "**stationary** — meaning constant mean, constant variance, no trend over time.\n",
    "\n",
    "If the data is non-stationary, the model will learn spurious patterns and forecast poorly.\n",
    "\n",
    "**Two complementary tests:**\n",
    "\n",
    "| Test | Null Hypothesis | Reject means |\n",
    "|------|----------------|---------------|\n",
    "| ADF  | Series has a unit root (non-stationary) | Series IS stationary |\n",
    "| KPSS | Series is stationary | Series is NOT stationary |\n",
    "\n",
    "We run both because they test different things. If both agree, we're confident.\n",
    "If they disagree, the series may be trend-stationary (needs differencing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "def run_adf_test(series, name=''):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'--- ADF Test: {name} ---')\n",
    "    print(f'ADF Statistic: {result[0]:.4f}')\n",
    "    print(f'p-value: {result[1]:.4f}')\n",
    "    for key, val in result[4].items():\n",
    "        print(f'  Critical Value ({key}): {val:.4f}')\n",
    "    if result[1] < 0.05:\n",
    "        print('→ REJECT null hypothesis: Series IS stationary')\n",
    "    else:\n",
    "        print('→ FAIL to reject: Series is NON-stationary')\n",
    "    print()\n",
    "\n",
    "def run_kpss_test(series, name=''):\n",
    "    result = kpss(series.dropna(), regression='ct')\n",
    "    print(f'--- KPSS Test: {name} ---')\n",
    "    print(f'KPSS Statistic: {result[0]:.4f}')\n",
    "    print(f'p-value: {result[1]:.4f}')\n",
    "    for key, val in result[3].items():\n",
    "        print(f'  Critical Value ({key}): {val:.4f}')\n",
    "    if result[1] < 0.05:\n",
    "        print('→ REJECT null hypothesis: Series is NON-stationary')\n",
    "    else:\n",
    "        print('→ FAIL to reject: Series IS stationary')\n",
    "    print()\n",
    "\n",
    "# Test on transaction volume (raw)\n",
    "run_adf_test(daily['transaction_volume'], 'Transaction Volume (raw)')\n",
    "run_kpss_test(daily['transaction_volume'], 'Transaction Volume (raw)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on net_flow (raw)\n",
    "run_adf_test(daily['net_flow'], 'Net Flow (raw)')\n",
    "run_kpss_test(daily['net_flow'], 'Net Flow (raw)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8",
   "metadata": {},
   "source": [
    "## Step 8: Making Data Stationary (if needed)\n",
    "\n",
    "**If the tests show non-stationarity, we apply differencing.**\n",
    "\n",
    "**What is differencing?**\n",
    "Instead of modeling the raw value Y(t), we model the CHANGE: Y(t) - Y(t-1).\n",
    "This removes trends because a constant upward trend becomes a constant number after differencing.\n",
    "\n",
    "**Why not log transform?** Log works when variance grows with the level (multiplicative).\n",
    "For transaction counts, variance is often additive, so differencing is usually sufficient.\n",
    "\n",
    "We then re-run stationarity tests to confirm differencing worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-order differencing\n",
    "volume_diff = daily['transaction_volume'].diff().dropna()\n",
    "net_flow_diff = daily['net_flow'].diff().dropna()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Original vs Differenced: Volume\n",
    "axes[0, 0].plot(daily['transaction_volume'], color='#FF914D', linewidth=0.8)\n",
    "axes[0, 0].set_title('Transaction Volume — Original')\n",
    "axes[0, 1].plot(volume_diff, color='#FF914D', linewidth=0.8)\n",
    "axes[0, 1].set_title('Transaction Volume — After 1st Differencing')\n",
    "\n",
    "# Original vs Differenced: Net Flow\n",
    "axes[1, 0].plot(daily['net_flow'], color='#1f77b4', linewidth=0.8)\n",
    "axes[1, 0].set_title('Net Flow — Original')\n",
    "axes[1, 1].plot(net_flow_diff, color='#1f77b4', linewidth=0.8)\n",
    "axes[1, 1].set_title('Net Flow — After 1st Differencing')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-test stationarity after differencing\n",
    "print('=== After 1st Differencing ===')\n",
    "run_adf_test(volume_diff, 'Transaction Volume (differenced)')\n",
    "run_kpss_test(volume_diff, 'Transaction Volume (differenced)')\n",
    "run_adf_test(net_flow_diff, 'Net Flow (differenced)')\n",
    "run_kpss_test(net_flow_diff, 'Net Flow (differenced)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c",
   "metadata": {},
   "source": [
    "### Ljung-Box Test — Is it White Noise or Random Walk?\n",
    "\n",
    "**Why:** After differencing, we need to check if the series still has autocorrelation\n",
    "(today's value is correlated with yesterday's). If it does → forecastable. If not → white noise\n",
    "(random, cannot be forecast).\n",
    "\n",
    "- **H0:** No autocorrelation (white noise)\n",
    "- **H1:** Autocorrelation exists (forecastable)\n",
    "- If p < 0.05 → reject H0 → data IS forecastable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Test on raw volume\n",
    "print('Ljung-Box Test: Transaction Volume (raw)')\n",
    "lb_raw = acorr_ljungbox(daily['transaction_volume'], lags=[7, 14, 30], return_df=True)\n",
    "print(lb_raw)\n",
    "\n",
    "print('\\nLjung-Box Test: Transaction Volume (differenced)')\n",
    "lb_diff = acorr_ljungbox(volume_diff, lags=[7, 14, 30], return_df=True)\n",
    "print(lb_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9",
   "metadata": {},
   "source": [
    "## Step 9: ACF & PACF Plots\n",
    "\n",
    "**Why:** These plots directly tell us the order of AR and MA components for ARIMA.\n",
    "\n",
    "- **ACF (Autocorrelation Function):** Shows correlation between Y(t) and Y(t-k) for each lag k.\n",
    "  - If ACF cuts off sharply after lag q → use MA(q)\n",
    "  - If ACF decays slowly → AR component is dominant\n",
    "\n",
    "- **PACF (Partial Autocorrelation Function):** Shows correlation between Y(t) and Y(t-k)\n",
    "  AFTER removing the effect of intermediate lags.\n",
    "  - If PACF cuts off sharply after lag p → use AR(p)\n",
    "  - If PACF decays slowly → MA component is dominant\n",
    "\n",
    "**Rules for ARIMA(p, d, q):**\n",
    "- d = number of differencings needed (from Step 8)\n",
    "- p = read from PACF (where it cuts off)\n",
    "- q = read from ACF (where it cuts off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# ACF & PACF on differenced transaction volume\n",
    "plot_acf(volume_diff, ax=axes[0, 0], lags=40, color='#FF914D')\n",
    "axes[0, 0].set_title('ACF — Transaction Volume (differenced)')\n",
    "\n",
    "plot_pacf(volume_diff, ax=axes[0, 1], lags=40, method='ywm', color='#FF914D')\n",
    "axes[0, 1].set_title('PACF — Transaction Volume (differenced)')\n",
    "\n",
    "# ACF & PACF on differenced net flow\n",
    "plot_acf(net_flow_diff, ax=axes[1, 0], lags=40, color='#1f77b4')\n",
    "axes[1, 0].set_title('ACF — Net Flow (differenced)')\n",
    "\n",
    "plot_pacf(net_flow_diff, ax=axes[1, 1], lags=40, method='ywm', color='#1f77b4')\n",
    "axes[1, 1].set_title('PACF — Net Flow (differenced)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10",
   "metadata": {},
   "source": [
    "## Step 10: Smoothing Methods — Trend Visualization\n",
    "\n",
    "**Why:** Smoothing removes short-term noise to reveal the underlying trend.\n",
    "This is not for forecasting itself — it's for understanding.\n",
    "\n",
    "**Three methods compared:**\n",
    "- **SMA (Simple Moving Average):** Equal weight to all days in window. Lags behind turns.\n",
    "- **WMA (Weighted Moving Average):** Recent days get higher weight. Reacts faster than SMA.\n",
    "- **EMA (Exponential Moving Average):** Exponentially decaying weights. Fastest response to recent changes.\n",
    "\n",
    "For our forecasting, these smoothed values will later become features (rolling_mean_7, rolling_mean_30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 7  # 7-day window (weekly smoothing)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.plot(daily['transaction_volume'], label='Raw Volume', color='lightgrey', linewidth=0.8)\n",
    "\n",
    "# SMA\n",
    "sma = daily['transaction_volume'].rolling(window=window).mean()\n",
    "plt.plot(sma, label=f'SMA ({window}-day)', color='green', linewidth=1.5)\n",
    "\n",
    "# WMA\n",
    "weights = np.arange(1, window + 1)\n",
    "wma = daily['transaction_volume'].rolling(window).apply(\n",
    "    lambda x: np.dot(x, weights) / weights.sum(), raw=True)\n",
    "plt.plot(wma, label=f'WMA ({window}-day)', color='orange', linewidth=1.5)\n",
    "\n",
    "# EMA\n",
    "ema = daily['transaction_volume'].ewm(span=window).mean()\n",
    "plt.plot(ema, label=f'EMA ({window}-day)', color='red', linewidth=1.5)\n",
    "\n",
    "plt.title('Transaction Volume — Smoothing Comparison (7-day window)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Transaction Volume')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also compare with 30-day smoothing for longer-term trend\n",
    "window_long = 30\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.plot(daily['transaction_volume'], label='Raw Volume', color='lightgrey', linewidth=0.8)\n",
    "\n",
    "sma_30 = daily['transaction_volume'].rolling(window=window_long).mean()\n",
    "plt.plot(sma_30, label=f'SMA ({window_long}-day)', color='green', linewidth=2)\n",
    "\n",
    "ema_30 = daily['transaction_volume'].ewm(span=window_long).mean()\n",
    "plt.plot(ema_30, label=f'EMA ({window_long}-day)', color='red', linewidth=2)\n",
    "\n",
    "plt.title('Transaction Volume — 30-Day Smoothing (Long-Term Trend)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Transaction Volume')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11",
   "metadata": {},
   "source": [
    "## Step 11: Resampling — Weekly Aggregation\n",
    "\n",
    "**Why:** The problem statement requires forecasting at both daily AND weekly levels.\n",
    "Weekly aggregation smooths out day-to-day noise, so weekly MAPE target is tighter (<10% vs <15%).\n",
    "\n",
    "Downsampling daily → weekly using sum for amounts/volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = daily.resample('W').agg({\n",
    "    'transaction_volume': 'sum',\n",
    "    'total_deposit': 'sum',\n",
    "    'total_withdrawal': 'sum',\n",
    "    'net_flow': 'sum',\n",
    "    'unique_accounts': 'max'\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "axes[0].plot(weekly['transaction_volume'], color='#FF914D', marker='o', markersize=3)\n",
    "axes[0].set_title('Weekly Transaction Volume')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(weekly['net_flow'], color='#1f77b4', marker='o', markersize=3)\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "axes[1].set_title('Weekly Net Flow (Liquidity Position)')\n",
    "axes[1].set_ylabel('Net Amount')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Weekly aggregated shape: {weekly.shape}')\n",
    "weekly.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12",
   "metadata": {},
   "source": [
    "## Step 12: Train / Validation / Test Split Strategy\n",
    "\n",
    "**Why time-based split (not random)?** In time series, future data must NEVER leak into training.\n",
    "A random 80/20 split would let the model see Dec data while predicting Oct — that's cheating.\n",
    "\n",
    "**Split:**\n",
    "- Train: Everything except last 60 days\n",
    "- Validation: Next 30 days (for hyperparameter tuning)\n",
    "- Test: Last 30 days (final evaluation, untouched until the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_days = 30\n",
    "val_days = 30\n",
    "\n",
    "train = daily.iloc[:-(test_days + val_days)]\n",
    "val = daily.iloc[-(test_days + val_days):-test_days]\n",
    "test = daily.iloc[-test_days:]\n",
    "\n",
    "print(f'Train: {train.index.min()} to {train.index.max()} ({len(train)} days)')\n",
    "print(f'Val:   {val.index.min()} to {val.index.max()} ({len(val)} days)')\n",
    "print(f'Test:  {test.index.min()} to {test.index.max()} ({len(test)} days)')\n",
    "\n",
    "# Visualize the split\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(train.index, train['transaction_volume'], color='blue', label='Train')\n",
    "plt.plot(val.index, val['transaction_volume'], color='orange', label='Validation')\n",
    "plt.plot(test.index, test['transaction_volume'], color='red', label='Test')\n",
    "plt.title('Train / Validation / Test Split — No Future Leakage')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13",
   "metadata": {},
   "source": [
    "## Step 13: Save Processed Data\n",
    "\n",
    "Save the aggregated daily and weekly datasets for use in modeling notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.to_csv('daily_aggregated.csv')\n",
    "weekly.to_csv('weekly_aggregated.csv')\n",
    "\n",
    "print('Saved: daily_aggregated.csv')\n",
    "print('Saved: weekly_aggregated.csv')\n",
    "print(f'\\nDaily columns: {list(daily.columns)}')\n",
    "print(f'Weekly columns: {list(weekly.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14",
   "metadata": {},
   "source": [
    "## EDA Summary & Key Findings\n",
    "\n",
    "| Step | What We Did | Why It Matters |\n",
    "|------|------------|----------------|\n",
    "| Data Cleaning | Dropped cheque_no, balance_amt; kept value_date for settlement | Correct aggregation date for liquidity |\n",
    "| Aggregation | Raw txns → daily volume, deposit, withdrawal, net_flow | Created our forecast targets |\n",
    "| Missing Dates | Reindexed to continuous calendar, filled with 0 | ARIMA needs continuous time index |\n",
    "| Visualization | Plotted trends, day-of-week, month-end patterns | Confirmed seasonality exists |\n",
    "| Decomposition | Classical + STL with period=7 | Separated trend, weekly seasonality, residuals |\n",
    "| Stationarity | ADF + KPSS tests | Determined if differencing is needed (d parameter) |\n",
    "| Differencing | First-order diff, then re-tested | Made series stationary for ARIMA |\n",
    "| Ljung-Box | Autocorrelation test | Confirmed data IS forecastable (not white noise) |\n",
    "| ACF/PACF | Plotted to identify p, q orders | Guides ARIMA(p,d,q) selection |\n",
    "| Smoothing | SMA, WMA, EMA at 7-day and 30-day | Revealed long-term trend direction |\n",
    "| Resampling | Daily → weekly aggregation | Secondary forecast target (tighter MAPE) |\n",
    "| Split | Time-based train/val/test | No data leakage guaranteed |\n",
    "\n",
    "**Next step:** Use these findings to build ARIMA/SARIMA baseline (Week 2 deliverable)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}